---
title: "Attrition at D.D.S. Analytics"
author: "Justin Howard"
date: "August 7, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Data is the buzz-word of the twenty-first century. The rapid expansion of predictive analytics to fields outside the finance and manufacturing industries is opening new opportunities in every field. At D.D.S. Analytics, we are embarking on a project that will enhance our ability to provide our clients with a deeper understanding of their human capital flows, beginning with attrition. Every journey begins with a single step, and our first charge is to understand our employees. We focus mainly on attrition and its accompanying trends, but will also include a brief but instructive analysis of our current patterns related to compensation. 

We explored the data to find surface-level trends using standard exploratory techniques and also used more advanced machine learning techniques to find hidden trends. We conducted a comparative review of three predictive classification models to choose the model that best identified the top three single factors that lead to attrition. Those factors and many other less important predictors had significant variance between their levels. To capture the influence of individual factor levels, we re-tooled the most predictive model to offer a more detailed, level-based analysis to examine which responses were the best predictors of attrition. 

-------------------------------------------------------------------------------------
Link to video presentation (please use Internet Explorer):
https://www.screencast.com/t/MsRfs5V5a

Link to GitHub repository:
https://www.github.com/juhoward/CaseStudy2DDS
-------------------------------------------------------------------------------------

# Data Description

The dataset examined has the following dimensions:

---------------------------------
Feature                    Result
------------------------ --------
Number of observations        870

Number of variables            36
---------------------------------


# Codebook Summary Table

-------------------------------------------------------------------------------------
Label   Variable                         Class       # unique  Missing  Description  
                                                       values                        
------- -------------------------------- --------- ---------- --------- -------------
        **[ID]**                         integer          870  0.00 %   Employee ID Number             

        **[Age]**                        integer           43  0.00 %   Employee Age             

        **[Attrition]**                  factor             2  0.00 %   "Yes" or "No" if employee left company               

        **[BusinessTravel]**             factor             3  0.00 %   Non-Travel, Travel_Frequently, Travel_Rarely           

        **[DailyRate]**                  integer          627  0.00 %   Daily pay              

        **[Department]**                 factor             3  0.00 %   Human Resources, Research & Dev., Sales             

        **[DistanceFromHome]**           integer           29  0.00 %   Commute distance             

        **[Education]**                  integer            5  0.00 %   Levels of education from 1-5           

        **[EducationField]**             factor             6  0.00 %   Degree field             

        **[EmployeeCount]**              integer            1  0.00 %   Number of employees             

        **[EmployeeNumber]**             integer          870  0.00 %   Employee number             

        **[EnvironmentSatisfaction]**    integer            4  0.00 %   Satisfaction rating from 1-4             

        **[Gender]**                     factor             2  0.00 %   Male or Female             

        **[HourlyRate]**                 integer           71  0.00 %   Hourly pay             

        **[JobInvolvement]**             integer            4  0.00 %   Employee's expressed involvement in job             

        **[JobLevel]**                   integer            5  0.00 %   Employee rank             

        **[JobRole]**                    factor             9  0.00 %   Job title             

        **[JobSatisfaction]**            integer            4  0.00 %   Employee's satisfaction rating from 1-4             

        **[MaritalStatus]**              factor             3  0.00 %   Marital status from Divorced, Married, Single             

        **[MonthlyIncome]**              integer          826  0.00 %   Monthly salary             

        **[MonthlyRate]**                integer          852  0.00 %   Monthly rate of pay             

        **[NumCompaniesWorked]**         integer           10  0.00 %   Number of companies worked for in the past             

        **[Over18]**                     factor             1  0.00 %   Yes for over 18             

        **[OverTime]**                   factor             2  0.00 %   Yes or No for having worked overtime             

        **[PercentSalaryHike]**          integer           15  0.00 %   Percentage of last raise in pay             

        **[PerformanceRating]**          integer            2  0.00 %   Job skill rating from 1-4             

        **[RelationshipSatisfaction]**   integer            4  0.00 %   Employee satisfaction with their manager             

        **[StandardHours]**              integer            1  0.00 %   Number of hours worked per week             

        **[StockOptionLevel]**           integer            4  0.00 %   Vestagure in company stock             

        **[TotalWorkingYears]**          integer           39  0.00 %   Total number of working years for employee             

        **[TrainingTimesLastYear]**      integer            7  0.00 %   Number of trainings attended within 1 year             

        **[WorkLifeBalance]**            integer            4  0.00 %   Employee rating from 1-4             

        **[YearsAtCompany]**             integer           32  0.00 %   Number of years at D.D.S. Analytics             

        **[YearsInCurrentRole]**         integer           19  0.00 %   Number of years in current position             

        **[YearsSinceLastPromotion]**    integer           16  0.00 %   Number of years since last promotion             

        **[YearsWithCurrManager]**       integer           17  0.00 %   Number of years working with manager             
-------------------------------------------------------------------------------------


```{r echo=F, include=F}
library(doParallel)
cores <- parallel::detectCores()
# [1] 8 for me, different for your machine

# Generally do one less
workers <- makeCluster(7L)

# register for parallel computation
registerDoParallel(workers)
```
```{r loading data, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(frequency)
library(dlookr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(corrplot)
library(cowplot)
library(purrr)
library(class)
library(caret)
library(car)
library(dataMaid)
library(readxl)
library(glmnet)
library(ROCR)
setwd('c:/users/howar/documents/r_wd/dds/casestudy2dds')
df1<- read.csv("CaseStudy2-data.csv")

df2<- read.csv("CaseStudy2CompSet No Attrition.csv")

df3<- read_excel("CaseStudy2Compset No Salary.xlsx")


str(df1)
```

```{r data structure exploration, include =F}
# make new dataset made of only useful variables
attrition<- df1[,-c(1, 10, 11, 23, 28)]
str(attrition)
# identify the categorical and continuous variables 
cats<- df1[, c(3, 4, 6, 8, 9, 12, 13, 15:19, 24, 26, 27,29, 32)]
cont<- df1[, c(3, 2, 5, 7, 14, 20, 21, 22, 25, 30, 31, 33:36)]

# make lists of column names to refer to
cat.names<- colnames(cats)
cont.names<- colnames(cont)

attrition.f<- attrition

# turn all categorical into factors for graphing and analysis
attrition.f[,cat.names]<- data.frame(lapply(attrition[,cat.names], factor))



# identify the predictors and the response
x.vars<- colnames(attrition[,-1])
y.var<- colnames(attrition[,1])


######################
### Salary for lm
######################

salary<- df1[,-c(1, 10, 11, 23, 28)]
salary[,cat.names]<- data.frame(lapply(salary[,cat.names], factor))

# preparing comparison set for external validation
external.sal<- df3[,-c(1, 10, 11, 22, 27)]
external.sal[,cat.names]<- data.frame(lapply(external.sal[,cat.names], factor))
```
```{r clean external validation set, include=F}
df2.0<- df2[,-c(1, 9, 10, 22, 27)]

df2.0[,cat.names[-1]]<- data.frame(lapply(df2.0[,cat.names[-1]], factor))
```

#Exploratory Data Analysis
###Validation of Assumptions

Given the nature of the binary response variable, Attrition, we choose to build a logistic regression model to fit the data. Logistic regression requires us to meet some primary assumptions. These are the assumptions of the outcome being binary, linearity as it relates between the logit of the outcome and each predictor, the addressing of influential values in continuous predictors, and no high multicollinearity. 

•	Binary Outcome – As discussed, we are attempting to predict whether the Attrition variable will be a "Yes" or "No" response. This is a binary outcome and will suffice to satisfy this assumption. 
•	Linearity – Reviewing the Appendix figure titled Linearity Assumption Checks we see a plot of a fully saturated model. From this we can see that there is curvature to some of these variables. This is expressed highly with fixed acidity, pH, and residual sugar. The other variables look ok in terms of linearity. Understanding this, we proceed with caution and for now assume this assumption met.
•	Influential Values – In reviewing the figure CooksD-Outliers, we see graphing of high leverage observations. We see three data points that stands out. These observations (id=1082, 4381, 5501) were reviewed, but no legitimate reason was found to exclude it from analysis. We left the data set intact.
•	Multi-collinearity – The saturate model was reviewed, and it was found that density had a high variable inflation factor (VIF) of 10, and alcohol had a VIF of 5. All other VIF were reasonable. We will address these collinearity issues during feature selection. 

**Descriptive Statistics**

Significant departures from normality appear prevalent across all variables. A series of density plots will be used for further examination. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

#Full dataset : Descriptive Statistics- sort by skewness
df1 %>%
  describe() %>%
  filter(!is.na(skewness)) %>% 
  arrange(desc(abs(skewness)))
```



###Examining Continuous Variables

We will proceed to examine both numerically coded and continuous variables for normality and homoscedasticity. Significant departures from normality are very clear across all continuous variables. Given the size of the dataset and the choice of logistic regression as the primary method of analysis, we will accept the departures in normality for the exmaination of attrition and use non-parametric classification techniques, such as KNN and Naive Bayes, that do not depend on the normal distribution. We will address these departures when we conduct our linear regression on Monthly Income.

```{r density plots, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
# assessing normality of the continuous variables
columns1<- colnames(cont) [2:9]
columns2<- colnames(cont) [10:15]
dens1 <- lapply(columns1, FUN=function(var) {
  ggplot(cont, aes_string(x=var)) + 
    geom_density(fill='gray') +
    geom_vline(aes(xintercept=mean(cont[,var])), color='blue', size=1) +
    geom_vline(aes(xintercept=median(cont[, var])), color='darkmagenta', size=1) +
    geom_vline(aes(xintercept=quantile(cont[, var], 0.25)), 
               linetype='dashed', size=0.5) + 
    geom_vline(aes(xintercept=quantile(cont[, var], 0.75)), 
               linetype='dashed', size=0.5)
})

dens2 <- lapply(columns2, FUN=function(var) {
  ggplot(cont, aes_string(x=var)) + 
    geom_density(fill='gray') +
    geom_vline(aes(xintercept=mean(cont[,var])), color='blue', size=1) +
    geom_vline(aes(xintercept=median(cont[, var])), color='darkmagenta', size=1) +
    geom_vline(aes(xintercept=quantile(cont[, var], 0.25)), 
               linetype='dashed', size=0.5) + 
    geom_vline(aes(xintercept=quantile(cont[, var], 0.75)), 
               linetype='dashed', size=0.5)
})

do.call(grid.arrange, args=c(dens1, list(ncol=3)))
do.call(grid.arrange, args=c(dens2, list(ncol=3)))
```

Linear relationships are difficult to ascertain except in the case of Age verses MonthlyIncome. A very clear linear relationship exists between the two variables. A clear, but less useful linear relationiship exists between YearsAtCompany, TotalWorkingYears and YearsInCurrentRole. A correlation matrix will be used to assess the Pearson's R correlations between these variables and we will proceed with multicollinearity problems in mind. 

```{r scatterplots, echo=F}
# assessing linearity of the continuous variables
attrition.numeric  <- attrition.f %>% keep(is.integer)
pairs(attrition.numeric[1:5], col=attrition.f$Attrition)
pairs(attrition.numeric[6:11], col=attrition.f$Attrition)
pairs(attrition.numeric[12:14], col=attrition.f$Attrition)

pairs(attrition.numeric[1:5], col=attrition.f$OverTime)
pairs(attrition.numeric[6:10], col=attrition.f$OverTime)
pairs(attrition.numeric[11:14], col=attrition.f$OverTime)
```

**A closer look at continuous variable relationships**

A strong linear relationship between Age and MonthlyIncome exists. Job Performance is negatively correlated with MonthlyIncome. MonthlyIncome does vary between departments and that variance will be examined for statistical significance in the salary regression. A clear relationship between the MonthlyIncome, Department and JobRole variables exists.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
plotdf<- df1
plotdf[,24]<- df1[,factor(24)]
plotdf[,24]<- data.frame(lapply(plotdf[,24], factor))

ggplot(plotdf, aes(x=Age, y=MonthlyIncome, col=as.factor(PerformanceRating))) + 
  geom_point()+
  geom_smooth(method=lm)+
  labs(title = "Compensation by Age") +
  theme(plot.title = element_text(hjust = .5)) +
  guides(fill= guide_legend(title= element_blank()))

ggplot(df1, aes(x=as.factor(PerformanceRating), y=MonthlyIncome, col=Age)) + 
  geom_point()+
  labs(title = "Compensation by Performance", x= "Performance Rating") +
  theme(plot.title = element_text(hjust = .5))
```

```{r analyzing categoricals, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(df1, aes(y=(MonthlyIncome/1000), x=Department, fill=Department)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) +
  coord_flip() + labs(title = "Compensation by Department", y = "Monthly Income / 1000") +
    theme(axis.title.y = element_blank(),  plot.title = element_text(hjust = .5))
  
ggplot(df1, aes(y=(MonthlyIncome/1000), x=JobRole, fill= JobRole)) +
  coord_flip() +
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)  +
    coord_flip() + labs(title = "Compensation by Job", y = "Monthly Income / 1000") +
    theme(axis.title.y = element_blank(),  plot.title = element_text(hjust = .5))
```


Signficant relationships exist between attrition and many of the following categorical variables:


```{r categorical graphs, echo=FALSE, message=FALSE, warning=FALSE}

lotsabars <- function(df, x,y){
  ggplot(data= df, aes_string(x = x, fill = y)) + 
    geom_bar(alpha = 0.9, position = "fill") +
    coord_flip() +
    labs(title = x, y = "Proportion") +
    theme(axis.title.y = element_blank(),  plot.title = element_text(hjust = .5))
}
# identifying response
yname  <-  "Attrition"

# isolating numeric and categoricals

attrition.cats  <- attrition.f %>% keep(is.factor)

# identifying predictors
xname  <-  names(attrition.cats[,-1])


plist <- lapply(xname, function(x) lotsabars(df = attrition.cats, x = x, y = yname))

plot_grid(plotlist = plist, ncol = 2)

lapply(xname, function(x) lotsabars(df = attrition.cats, x = x, y = yname))
```

Signficant relationships are visualized between OverTime and the following categorical variables:

```{r plots of particular interest, echo=FALSE, message=FALSE, warning=FALSE}
yovertime<- "OverTime"
lapply(xname, function(x) lotsabars(df = attrition.cats, x = x, y = yovertime))
```

Multicollinearity issues are present throughout the data and are especially strong among Age, YearsWithCurrManager, YearsAtCompany, YearsInCurrentRole and TotalWorkingyears. Our regression model will require penalty terms to compensate for the strong correlations indicated here. 

```{r correlation plot, echo=FALSE, message=FALSE, warning=FALSE}
# correlation function for numeric and continuous predictors
correlator  <-  function(df){
	df %>%
		keep(is.numeric) %>%
		tidyr::drop_na() %>%
		cor %>%
		corrplot( addCoef.col = "white", number.digits = 2,
			 number.cex = 0.5, method="square",
			 order="hclust",
			 tl.srt=45, tl.cex = 0.8)
}

correlator(attrition.numeric)
```
```{r correlations of interest, echo=FALSE, message=FALSE, warning=FALSE}
interest<- attrition %>%
  select(c("OverTime", "MaritalStatus"))
interest<- sapply(interest, function(x) as.numeric(x))
interest.plus<- cbind(interest, attrition.numeric)

correlator(interest.plus)
boring<- c("DistanceFromHome", "HourlyRate", "DailyRate", "Education", "NumCompaniesWorked", "EnvironmentSatisfaction", "WorkLifeBalance","RelationshipSatisfaction", "JobLevel")

dropout<- function(df, x) {
  droplist<- x
  soCool<<- df[,!colnames(df) %in% droplist]
}

dropout(interest.plus, boring)

correlator(soCool)
```
```{r train test set split,echo=FALSE, message=FALSE, warning=FALSE}
# changing nominal factors to ordinal factors so algorithms work.
columns<- c(2, 3, 5, 8, 10, 14, 16, 20)
attrition[, columns]<- sapply(attrition[, columns], as.numeric)
#creating train-test set split of 70:30
set.seed(1)
index<- sample(1:nrow(attrition), .7*nrow(attrition))
train<- attrition[index,]
test<- attrition[-index,]

labels.train<- train$Attrition
labels.test<- test$Attrition

# creating alternate training and test sets true to factor status of all variables
set.seed(1)
index.f<- sample(1:nrow(attrition.f), .7*nrow(attrition.f))
train.f<- attrition.f[index.f,]
test.f<- attrition.f[-index.f,]

f.labels.train<- train.f$Attrition
f.labels.test<- test.f$Attrition
```
```{r KNN}
for (k in 1:20) {
  print(k)
  # don't forget to remove response!
  predicted.labels<- knn(train[,-2], test[,-2], labels.train, k)
  num.incorrect.labels<- sum(predicted.labels != labels.test)
  misclassification.rate<- num.incorrect.labels / 
                            length(labels.test)
  print(misclassification.rate)
}
## The best K is 13.
# don't forget to remove response!
predictions.knn.final<- knn(train[,-2], test[,-2], labels.train, 13)

confusionMatrix(predictions.knn.final, as.factor(labels.test))
```
```{r logistic with levels as integers}

####### to provide summariezed results for a better presentation, we will run a dataset that
####### codes the numeric categorical variables as integer values 

### make new df for variables modified as a matrix
glm.attrition<- data.matrix(data.frame(attrition.f[,-2]))

set.seed(1)
glm.index.i<- sample(1:nrow(glm.attrition), .7*nrow(glm.attrition))
glm.train.i<- glm.attrition[glm.index.i,]
glm.test.i<- glm.attrition[-glm.index.i,]

# make training and test label matrices
m.labels.train<- data.matrix(attrition.f[glm.index.i, 2])
m.labels.test<- data.matrix(attrition.f[-glm.index.i, 2])

### use cross validatoin to find the best lambda value
set.seed(1)
test.cvfit.i<- cv.glmnet(glm.train.i, m.labels.train, 
                       family = 'binomial',
                       type.measure = 'class',
                       nlambda = 1000
                       ) 
set.seed(1)
logit.model.i<- glmnet(glm.train.i, m.labels.train,
                     alpha = 1,
                     family = "binomial",
                     lambda = test.cvfit.i$lambda.min)

logit.model.i$beta
plot(test.cvfit.i)
```

Top 3 factors leading to attrition
1. Working overtime
2. Marital Status
3. Job Involvement 

```{r logistic model 1 predictions, echo=FALSE, message=FALSE, warning=FALSE}

fit.pred<- predict(logit.model.i, newx = glm.test.i, type= 'response')
pred<- prediction(fit.pred[,1], f.labels.test)
roc.perf<- performance(pred, measure = 'tpr', x.measure = 'fpr')
auc.train<- performance (pred, measure = 'auc')
auc.train<- auc.train@y.values
```
```{r ROC curve, confusion matrix, echo=FALSE, message=FALSE, warning=FALSE}
plot(roc.perf,main="LASSO ROC")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))


#build confusion matrix
confusion.matrix <- table( fit.pred>0.5, f.labels.test )


# Performance analysis
tn <- confusion.matrix[1]
tp <- confusion.matrix[4]
fp <- confusion.matrix[3]
fn <- confusion.matrix[2]
paste("Accuracy")
(accuracy <- (tp + tn) / (tp + tn + fp + fn))
paste("Sensitivity")
(sensitivity<- (tp / (tp + fn)))
paste("Specificity")
(specificity<- (tn / (tn + fp)))
paste("Misclassification Rate")
(misclassification.rate <- 1 - accuracy)
paste("Null Error Rate")
(null.error.rate <- 1 - (tn / (tp + tn + fp + fn)))
paste("F-Score")
(FScore <- 2 * sensitivity * specificity / (sensitivity + specificity))
confusion.matrix

```
```{r logistic with true categories}

####### glmnet can't handle categoricals, even in numeric form, it treats them as integers.
####### we must make dummy variables for all categoricals using the model.matrix function.
####### this means we must first pre-standardizing integer values so we can turn off glmnet's 
####### stardardize option.


### identify names of integer variables
integers<- attrition.f %>%
  keep(is.integer)
int.names<- colnames(integers)

glm.attrition<- attrition.f
### scale integer variables
glm.attrition[,int.names]<- lapply(attrition.f[,int.names], scale)

################### make formula object
####get variables names
glm.attrition.1<- glm.attrition
xnames<- colnames(glm.attrition.1)

### make formula object
form<- as.formula(paste("Attrition ~ ", paste(xnames[-2], collapse = "+")))

############# create model matrix to make categorical variables into dummy variables
glm.attrition<- model.matrix(form, model.frame(glm.attrition))

set.seed(1)
glm.index<- sample(1:nrow(glm.attrition), .7*nrow(glm.attrition))
glm.train<- glm.attrition[glm.index,]
glm.test<- glm.attrition[-glm.index,]


### use cross validatoin to find the best lambda value
set.seed(1)
test.cvfit<- cv.glmnet(glm.train, m.labels.train, 
                       family = 'binomial',
                       type.measure = 'class',
                       nlambda = 1000,
                       standardize = F) #be sure to set to FALSE since we already standardized
set.seed(1)
logit.model<- glmnet(glm.train, m.labels.train,
                     alpha = 1,
                     family = "binomial",
                     standardize = F,
                     lambda = test.cvfit$lambda.min)

logit.model$beta
plot(test.cvfit)
```

```{r logistic model 1 .predictions, echo=FALSE, message=FALSE, warning=FALSE}

fit.pred.2<- predict(logit.model, newx = glm.test, type= 'response')
pred.2<- prediction(fit.pred.2[,1], f.labels.test)
roc.perf.2<- performance(pred.2, measure = 'tpr', x.measure = 'fpr')
auc.train.2<- performance (pred.2, measure = 'auc')
auc.train.2<- auc.train.2@y.values
```

```{r Accuracy, echo=FALSE, message=FALSE, warning=FALSE}
plot(roc.perf.2,main="LASSO ROC")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train.2[[1]],3), sep = ""))


#build confusion matrix
confusion.matrix <- table( fit.pred.2>0.5, f.labels.test )


# Performance analysis
tn <- confusion.matrix[1]
tp <- confusion.matrix[4]
fp <- confusion.matrix[3]
fn <- confusion.matrix[2]

(accuracy <- (tp + tn) / (tp + tn + fp + fn))
(sensitivity<- (tp / (tp + fn)))
(specificity<- (tn / (tn + fp)))
(misclassification.rate <- 1 - accuracy)
(recall <- tp / (tp + fn))
(precision <- tp / (tp + fp))
(null.error.rate <- tn / (tp + tn + fp + fn))
(FScore <- 2 * precision * recall / (precision + recall))
confusion.matrix
```
```{r logistic predictions}

### transform external validation set using model.matrix
### logit.model needs the same dimensions as newx
### first, make a new formula object because the validation set is missing the repsonse

### make formula object
#form.2<- as.formula(paste(". ~ ", paste(xnames, collapse = "+")))

#df2.0$Attrition<- attrition.f$Attrition[300]

#glm.df2.0<- model.matrix(form, data.frame(df2.0))
#data.matrix(data.frame(attrition.f[,-2]))
df2.0<- data.matrix(data.frame(df2.0))
log.final.preds<- predict(logit.model.i, newx= df2.0, type = 'response')

binary<- function(x) {
  x<- as.factor(ifelse(x > .5, "Yes", "No"))
}

attrition.preds<- sapply(log.final.preds, binary)
write.csv(attrition.preds, "Case2PredictionsHoward Attrition.csv")
```
```{r KNN caret}
# caret version of knn classifier must have nominal factors.
#ttrition.f<- df1[,-c(1, 10, 11, 23, 28)]
# creating new training set with nominal factors
#set.seed(1)
#index<- sample(1:nrow(attrition.f), .7*nrow(attrition.f))
#train.f<- attrition.f[index,]
#test.f<- attrition.f[-index,]

trainMethod <- trainControl(
  method = "repeatedcv",
  number = 25,
  repeats = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE)

fit.knn <- train(Attrition ~ .,
                data = train.f, # don't have to remove response
                method = "knn",
                metric = "Spec",
                trControl = trainMethod)

fit.knn$results
```
```{r naive bayes}

trainMethod <- trainControl(
  method = "repeatedcv",
  number = 25,
  repeats = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE)

fit.nb <- train(Attrition ~ .,
                data = train.f,  # don't have to remove response
                method = "nb",
                metric = "Spec",
                trControl = trainMethod)
summary(fit.nb)
fit.nb$results
```
```{r Regression Data prep}
# preparing data for linear regression with monthly salary as a response
set.seed(1)
index.sal<- sample(1:nrow(salary), .7*nrow(salary))
train.sal<- data.matrix(salary[index.sal,])
test.sal<- salary[-index.sal,]

train_x_sal<- data.matrix(train.sal[,-17])
train_y_sal<- train.sal[,17]

test_x_sal<- data.matrix(test[, -17])
test_y_sal<- data.matrix(test[,17])


```
```{r regression model, Elastic Net}
set.seed(1)
trainMethod <- trainControl(
  method = "repeatedcv",
  number = 25,
  repeats = 5,
  summaryFunction = defaultSummary
  )

#glm.train, m.labels.train
set.seed(1)
fit.reg <- train(MonthlyIncome ~ ., #removing monthly income from predictor set
                data = train.sal, # don't have to remove response
                method = "glmnet",
                metric = "RMSE",
                trControl = trainMethod)

fit.reg

# best alpha = .55
# best lambda = 87.660789
# final model is an Elastic Net model
coef(fit.reg)
reg.pred<- predict.train(fit.reg, newdata = test_x_sal)

residuals<- resid(fit.reg$finalModel)
plot(train$MonthlyIncome, residuals)



RMSE(test_y_sal, reg.pred)

coef(fit.reg$finalModel, fit.reg$bestTune$lambda)


external.pred<- predict.train(fit.reg, newdata = data.matrix(external.sal))
summary(external.pred)
write.csv(external.pred, "Case2PredictionsHoward Salary.csv")
```
